{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFm5A9vHx-Ku"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers torchinfo jedi datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogQviapJx-Kw"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/llava-hf/llava-1.5-7b-hf\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G_Ac0Whix-Kz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90ececdf96134870be4647ba2c41a198",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-1.5-7b-hf\", use_fast=True)\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"/teamspace/studios/this_studio/Hko2q4tVGjfzUZDUNH2RsA_b.jpg\"},\n",
        "            {\"type\": \"text\", \"text\": \"which animal is this?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "pipe(text=messages)\n",
        "\n",
        "print(pipe.device) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PTVoa0O-x-K0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9a3f66064784253aaaff2f8f6317d2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a dog.</s>\n"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=True)\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"/teamspace/studios/this_studio/Hko2q4tVGjfzUZDUNH2RsA_b.jpg\"},\n",
        "            {\"type\": \"text\", \"text\": \"which animal is this?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "inputs = processor.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- print the last layer \n",
        "- freeze all the layers first\n",
        "- unfreeze the last layer \n",
        "- add another layer, duplicate of the last layer \n",
        "- check a compatible model data set \n",
        "- train the model using that dataset\n",
        "- congrats you transfer learning completed using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "with open('model.txt', 'a') as f:\n",
        "    f.write(str(model)) \n",
        "    f.write(str(model.__dir__()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "with open(\"model_summary_brefore_training.txt\", 'w') as f:\n",
        "    f.write(str(summary(model, depth=10, verbose=0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoImageProcessor, LlavaForConditionalGeneration\n",
        "\n",
        "# Use separate components for granular control\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=True)\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "tokenizer.pad_token = tokenizer.eos_token # Critical Fix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "946481d40aee468e9c45b8217178ac5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load base model\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### step A: Freeze Vision, Unfreeze Projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfreezing Projector layers...\n",
            "  -> Unfrozen: model.multi_modal_projector.linear_1.weight\n",
            "  -> Unfrozen: model.multi_modal_projector.linear_1.bias\n",
            "  -> Unfrozen: model.multi_modal_projector.linear_2.weight\n",
            "  -> Unfrozen: model.multi_modal_projector.linear_2.bias\n",
            "\n",
            "Summary:\n",
            "Total Parameters: 7,063,427,072\n",
            "Trainable Parameters: 20,979,712\n",
            "Percentage Trainable: 0.30%\n"
          ]
        }
      ],
      "source": [
        "# 1. Start by freezing EVERYTHING (Base state)\n",
        "# This locks the Vision Encoder (layer 2-1) and the LlamaModel (layer 2-3)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. Unfreeze ONLY the Projector (Layer 2-2)\n",
        "# We iterate through parameters and unlock those belonging to the projector.\n",
        "# In most Hugging Face LLaVA implementations, these layers contain the keyword \"projector\" \n",
        "# (often named 'mm_projector' or 'multi_modal_projector').\n",
        "\n",
        "print(\"Unfreezing Projector layers...\")\n",
        "for name, param in model.named_parameters():\n",
        "    if \"projector\" in name:\n",
        "        param.requires_grad = True\n",
        "        print(f\"  -> Unfrozen: {name}\")\n",
        "\n",
        "# 3. Verification\n",
        "# Count how many parameters are actually trainable now.\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\") \n",
        "print(f\"Percentage Trainable: {100 * trainable_params / total_params:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step B: Applying LoRA and unfreezing the multi modal projector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting LoRA adapters and projector...\n",
            "trainable params: 30,941,184 || all params: 7,094,368,256 || trainable%: 0.4361\n"
          ]
        }
      ],
      "source": [
        "try: \n",
        "    from peft import LoraConfig, get_peft_model\n",
        "except:\n",
        "    !pip install peft\n",
        "    from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 1. Define the LoRA Configuration\n",
        "# We target the attention mechanisms of the LLM backbone.\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                       # Rank: Higher = more params, \"smarter\" but slower. 16 is standard.\n",
        "    lora_alpha=32,              # Scaling factor: Usually set to 2x the rank.\n",
        "    target_modules=[            # The specific layers to inject adapters into\n",
        "        \"q_proj\",               # Query projection in LlamaAttention\n",
        "        \"v_proj\"                # Value projection in LlamaAttention\n",
        "    ],\n",
        "    lora_dropout=0.05,          # Reduces overfitting\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",      # LLaVA behaves like a standard Causal LLM during training\n",
        "    modules_to_save=['multi_modal_projector']\n",
        ")\n",
        "\n",
        "# 2. Apply LoRA to the Model\n",
        "# This wraps the frozen layers with the new trainable LoRA adapters.\n",
        "print(\"Injecting LoRA adapters and projector...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 3. Verify the Trainable Parameters\n",
        "# You should see a slight increase in trainable params compared to Step A,\n",
        "# but it will still be < 2% of the total model.\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training \n",
        "#### loading Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "class ScienceQADataset(Dataset):\n",
        "    def __init__(self, split, tokenizer, image_processor):\n",
        "        # 1. Load the dataset from Hugging Face\n",
        "        print(f\"Loading ScienceQA [{split}]...\")\n",
        "        self.data = load_dataset(\"derek-thomas/ScienceQA\", split=split)\n",
        "        \n",
        "        # 2. Filter for rows WITH images only (Critical for Vision Transfer Learning)\n",
        "        # We assume you want to train the vision encoder/projector.\n",
        "        self.data = self.data.filter(lambda x: x['image'] is not None)\n",
        "        print(f\"Filtered to {len(self.data)} multimodal examples.\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_processor = image_processor\n",
        "        self.options = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        # --- A. Prepare Image ---\n",
        "        # The 'image' field is already a PIL object in this dataset\n",
        "        image = item['image'].convert('RGB')\n",
        "        image_tensor = self.image_processor(image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
        "\n",
        "        # --- B. Prepare Text (Question & Options) ---\n",
        "        question = item['question']\n",
        "        choices = item['choices']\n",
        "        \n",
        "        # Format choices like: \"(A) Choice 1  (B) Choice 2\"\n",
        "        choice_str = \" \".join([f\"{self.options[i]} {c}\" for i, c in enumerate(choices)])\n",
        "        \n",
        "        # Create the USER prompt\n",
        "        # Standard LLaVA format: USER: <image>\\n<prompt> ASSISTANT:\n",
        "        human_input = f\"<image>\\n{question}\\nOptions: {choice_str}\\nAnswer with the option letter.\"\n",
        "\n",
        "        # --- C. Prepare Answer ---\n",
        "        # Convert integer answer (e.g., 0) to Letter (e.g., \"A\") and add the explanation\n",
        "        answer_idx = item['answer']\n",
        "        answer_letter = self.options[answer_idx]\n",
        "        solution = item['solution'] if item['solution'] else \"\"\n",
        "        \n",
        "        gpt_response = f\"{answer_letter}. {solution}\"\n",
        "\n",
        "        # --- D. Tokenize ---\n",
        "        # Combine into full conversation for training\n",
        "        formatted_prompt = f\"USER: {human_input} ASSISTANT: {gpt_response}</s>\"\n",
        "\n",
        "        tokenized = self.tokenizer(\n",
        "            formatted_prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512, # Reduce to 256 if you hit OOM errors\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        input_ids = tokenized.input_ids.squeeze(0)\n",
        "        attention_mask = tokenized.attention_mask.squeeze(0)\n",
        "        labels = input_ids.clone()\n",
        "        \n",
        "        # Mask out padding tokens so they don't affect loss\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": image_tensor,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "# --- Usage ---\n",
        "# Assuming you already have 'tokenizer' and 'image_processor' from previous steps\n",
        "train_dataset = ScienceQADataset(\"train\", tokenizer, image_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# 1. Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llava-scienceqa-finetune\",\n",
        "    per_device_train_batch_size=4,  # Start low (e.g., 2 or 4) for 7B models\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n",
        "    learning_rate=2e-4,             # Standard LoRA learning rate\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,                      # Use mixed precision (save VRAM)\n",
        "    remove_unused_columns=False     # Important for custom LLaVA datasets\n",
        ")\n",
        "\n",
        "# 2. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                    # Your model with LoRA + Unfrozen Projector\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,    # The ScienceQA dataset we just built\n",
        "    # No \"eval_dataset\" needed for a quick test run\n",
        ")\n",
        "\n",
        "# 3. Start Training\n",
        "# trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
