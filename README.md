# Transfer-learning-on-LLaVA
Transfer Learning on LLaVA â€” This project applies transfer learning on the LLaVA (Large Language and Vision Assistant) multimodal model to train it for a new custom dataset and domain-specific tasks. The repository includes training scripts, dataset preprocessing, fine-tuning pipelines, evaluation metrics, and inference.
 ðŸš€ Transfer Learning on LLaVA (Large Language and Vision Assistant)

This repository demonstrates **transfer learning / fine-tuning** on **LLaVA**, a powerful multimodal AI model that combines **vision understanding + language reasoning**.  
The goal of this project is to adapt LLaVA to a **custom dataset** and improve performance on domain-specific multimodal tasks.

---

## ðŸ“Œ Highlights

- âœ… Fine-tuning LLaVA using **LoRA / QLoRA**
- âœ… Support for **custom imageâ€“text datasets**
- âœ… Full training pipeline (Lightning / PyTorch)
- âœ… Image captioning + Visual Question Answering (VQA)
- âœ… Inference demos for real-world usage
- âœ… Modular and clean code structure

---

##  What is LLaVA?

**LLaVA (Large Language and Vision Assistant)** is a multimodal model that:
- Understands images  
- Reads text  
- Answers questions  
- Generates descriptions  
- Performs visual reasoning

This makes it ideal for:
- Product classification  
- Medical imaging  
- Agriculture images  
- OCR tasks  
- VQA datasets  
- Custom domain visual analysis  

---


